#!/usr/bin/env python3
"""
é¡¹ç›®è‡ªåŠ¨æ¸…ç†è„šæœ¬
åŒ…æ‹¬ç»“æœæ–‡ä»¶å½’æ¡£ã€æ—¥å¿—è½®è½¬ã€ä¸´æ—¶æ–‡ä»¶æ¸…ç†ç­‰åŠŸèƒ½
"""

import os
import sys
import glob
import shutil
from datetime import datetime, timedelta

def cleanup():
    """æ‰§è¡Œæ‰€æœ‰æ¸…ç†ä»»åŠ¡"""

    print("ğŸš€ å¼€å§‹é¡¹ç›®æ¸…ç†...")
    print("=" * 50)

    # 1. å½’æ¡£æ—§çš„ç»“æœæ–‡ä»¶
    print("\nğŸ“ å½’æ¡£ç»“æœæ–‡ä»¶...")
    try:
        from scripts.archive_old_results import archive_old_results
        archive_old_results()
    except ImportError:
        print("âš ï¸ æ— æ³•å¯¼å…¥å½’æ¡£æ¨¡å—ï¼Œè·³è¿‡")

    # 2. è½®è½¬æ—¥å¿—æ–‡ä»¶
    print("\nğŸ“ è½®è½¬æ—¥å¿—æ–‡ä»¶...")
    try:
        from scripts.rotate_logs import rotate_log_file, cleanup_old_logs
        rotate_log_file("auto_scheduler.log")
        cleanup_old_logs(30)
    except ImportError:
        print("âš ï¸ æ— æ³•å¯¼å…¥æ—¥å¿—è½®è½¬æ¨¡å—ï¼Œè·³è¿‡")

    # 3. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
    print("\nğŸ—‘ï¸ æ¸…ç†ä¸´æ—¶æ–‡ä»¶...")
    temp_patterns = [
        "*.tmp",
        "*.temp",
        "*.log.*",
        "test_*.json",
        "debug_*.txt"
    ]

    cleaned_count = 0
    for pattern in temp_patterns:
        for file_path in glob.glob(pattern):
            try:
                if os.path.isfile(file_path):
                    os.remove(file_path)
                    cleaned_count += 1
                    print(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
            except OSError as e:
                print(f"æ— æ³•åˆ é™¤ {file_path}: {e}")

    # 4. æ¸…ç†ç©ºçš„__pycache__ç›®å½•
    print("\nğŸ æ¸…ç†Pythonç¼“å­˜...")
    cleaned_dirs = 0
    for root, dirs, files in os.walk("."):
        if "__pycache__" in dirs:
            pycache_path = os.path.join(root, "__pycache__")
            try:
                if not os.listdir(pycache_path):  # ç›®å½•ä¸ºç©º
                    os.rmdir(pycache_path)
                    cleaned_dirs += 1
                    print(f"åˆ é™¤ç©ºç¼“å­˜ç›®å½•: {pycache_path}")
                else:
                    # éç©ºç›®å½•ï¼Œåˆ é™¤.pycæ–‡ä»¶
                    pyc_files = glob.glob(os.path.join(pycache_path, "*.pyc"))
                    for pyc_file in pyc_files:
                        os.remove(pyc_file)
                        cleaned_count += 1
            except OSError as e:
                print(f"æ¸…ç†ç¼“å­˜å¤±è´¥ {pycache_path}: {e}")

    # 5. æ˜¾ç¤ºæ¸…ç†ç»Ÿè®¡
    print("\n" + "=" * 50)
    print("ğŸ“Š æ¸…ç†ç»Ÿè®¡:")
    print(f"   åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {cleaned_count} ä¸ª")
    print(f"   æ¸…ç†ç¼“å­˜ç›®å½•: {cleaned_dirs} ä¸ª")

    # 6. æ˜¾ç¤ºå½“å‰é¡¹ç›®çŠ¶æ€
    print("\nğŸ“ˆ å½“å‰é¡¹ç›®çŠ¶æ€:")

    # ç»Ÿè®¡ç»“æœæ–‡ä»¶
    result_files = glob.glob("enhanced_1130_result_*.json")
    archived_files = glob.glob("archive/results/*.json")
    print(f"   å½“å‰ç»“æœæ–‡ä»¶: {len(result_files)} ä¸ª")
    print(f"   å½’æ¡£ç»“æœæ–‡ä»¶: {len(archived_files)} ä¸ª")

    # ç»Ÿè®¡æ—¥å¿—æ–‡ä»¶
    if os.path.exists("auto_scheduler.log"):
        log_size = os.path.getsize("auto_scheduler.log") / (1024 * 1024)
        print(f"   å½“å‰æ—¥å¿—å¤§å°: {log_size:.2f} MB")

    log_files = glob.glob("archive/logs/*.gz")
    print(f"   å½’æ¡£æ—¥å¿—æ–‡ä»¶: {len(log_files)} ä¸ª")

    print("\nâœ… é¡¹ç›®æ¸…ç†å®Œæˆï¼")

if __name__ == "__main__":
    cleanup()
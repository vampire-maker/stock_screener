#!/usr/bin/env python3
"""
ç³»ç»Ÿæ¸…ç†å·¥å…·
è‡ªåŠ¨æ¸…ç†ç¼“å­˜æ–‡ä»¶ã€æ—¥å¿—æ–‡ä»¶å’Œä¸´æ—¶æ•°æ®
"""

import os
import sys
import shutil
import glob
import json
import time
import logging
import tarfile
from datetime import datetime, timedelta
from pathlib import Path

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°è·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root / 'src'))

try:
    from config import get_config
    config = get_config()
except ImportError:
    config = None

# ç¡®ä¿æ—¥å¿—ç›®å½•å­˜åœ¨
logs_dir = project_root / 'logs'
logs_dir.mkdir(exist_ok=True)

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(logs_dir / 'system_cleanup.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class SystemCleanup:
    """ç³»ç»Ÿæ¸…ç†å·¥å…·"""

    def __init__(self):
        self.project_root = project_root
        self.archive_dir = self.project_root / 'archive'
        self.logs_dir = self.project_root / 'logs'
        self.results_dir = self.project_root / 'results'

        # åˆ›å»ºå¿…è¦çš„ç›®å½•
        self.archive_dir.mkdir(exist_ok=True)
        self.logs_dir.mkdir(exist_ok=True)
        self.results_dir.mkdir(exist_ok=True)

        # æ¸…ç†é…ç½®
        self.cleanup_config = {
            'cache_retention_days': 7,
            'log_retention_days': 30,
            'result_retention_days': 90,
            'temp_file_retention_hours': 24
        }

    def clean_python_cache(self):
        """æ¸…ç†Pythonç¼“å­˜æ–‡ä»¶"""
        logger.info("å¼€å§‹æ¸…ç†Pythonç¼“å­˜æ–‡ä»¶...")

        cache_patterns = [
            '**/__pycache__/',
            '**/*.pyc',
            '**/*.pyo',
            '**/*.pyd'
        ]

        cleaned_count = 0
        for pattern in cache_patterns:
            for item in self.project_root.glob(pattern):
                try:
                    if item.is_dir():
                        shutil.rmtree(item)
                        logger.info(f"åˆ é™¤ç¼“å­˜ç›®å½•: {item}")
                    else:
                        item.unlink()
                        logger.info(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶: {item}")
                    cleaned_count += 1
                except Exception as e:
                    logger.warning(f"åˆ é™¤ç¼“å­˜å¤±è´¥ {item}: {e}")

        logger.info(f"Pythonç¼“å­˜æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {cleaned_count} é¡¹")
        return cleaned_count

    def clean_old_results(self, retention_days=None):
        """å½’æ¡£å’Œæ¸…ç†æ—§çš„ç»“æœæ–‡ä»¶"""
        retention_days = retention_days or self.cleanup_config['result_retention_days']
        cutoff_date = datetime.now() - timedelta(days=retention_days)

        logger.info(f"å¼€å§‹å½’æ¡£ {retention_days} å¤©å‰çš„ç»“æœæ–‡ä»¶...")

        # åˆ›å»ºå½’æ¡£å­ç›®å½•
        old_results_dir = self.archive_dir / 'old_results'
        old_results_dir.mkdir(exist_ok=True)

        result_patterns = [
            '*result*.json',
            '*screening*.json',
            '*burial*.json'
        ]

        archived_count = 0
        for pattern in result_patterns:
            for file_path in self.results_dir.glob(pattern):
                try:
                    # è·å–æ–‡ä»¶ä¿®æ”¹æ—¶é—´
                    file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)

                    if file_mtime < cutoff_date:
                        # åˆ›å»ºå½’æ¡£æ–‡ä»¶å
                        archive_name = f"{file_mtime.strftime('%Y%m')}/{file_path.name}"
                        archive_path = old_results_dir / archive_name

                        # åˆ›å»ºæœˆåº¦ç›®å½•
                        archive_path.parent.mkdir(exist_ok=True)

                        # ç§»åŠ¨æ–‡ä»¶åˆ°å½’æ¡£ç›®å½•
                        shutil.move(str(file_path), str(archive_path))
                        logger.info(f"å½’æ¡£ç»“æœæ–‡ä»¶: {file_path.name} -> {archive_name}")
                        archived_count += 1

                except Exception as e:
                    logger.warning(f"å½’æ¡£ç»“æœæ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        logger.info(f"ç»“æœæ–‡ä»¶å½’æ¡£å®Œæˆï¼Œå…±å½’æ¡£ {archived_count} ä¸ªæ–‡ä»¶")
        return archived_count

    def clean_old_logs(self, retention_days=None):
        """å‹ç¼©å’Œæ¸…ç†æ—§çš„æ—¥å¿—æ–‡ä»¶"""
        retention_days = retention_days or self.cleanup_config['log_retention_days']
        cutoff_date = datetime.now() - timedelta(days=retention_days)

        logger.info(f"å¼€å§‹å‹ç¼© {retention_days} å¤©å‰çš„æ—¥å¿—æ–‡ä»¶...")

        old_logs_dir = self.archive_dir / 'old_logs'
        old_logs_dir.mkdir(exist_ok=True)

        compressed_count = 0
        for log_file in self.logs_dir.glob('*.log'):
            try:
                file_mtime = datetime.fromtimestamp(log_file.stat().st_mtime)

                if file_mtime < cutoff_date:
                    # åˆ›å»ºå‹ç¼©æ–‡ä»¶
                    archive_name = f"{log_file.stem}_{file_mtime.strftime('%Y%m')}.tar.gz"
                    archive_path = old_logs_dir / archive_name

                    # å‹ç¼©æ—¥å¿—æ–‡ä»¶
                    with tarfile.open(archive_path, 'w:gz') as tar:
                        tar.add(log_file, arcname=log_file.name)

                    # åˆ é™¤åŸæ–‡ä»¶
                    log_file.unlink()
                    logger.info(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶: {log_file.name} -> {archive_name}")
                    compressed_count += 1

            except Exception as e:
                logger.warning(f"å‹ç¼©æ—¥å¿—æ–‡ä»¶å¤±è´¥ {log_file}: {e}")

        logger.info(f"æ—¥å¿—æ–‡ä»¶å‹ç¼©å®Œæˆï¼Œå…±å‹ç¼© {compressed_count} ä¸ªæ–‡ä»¶")
        return compressed_count

    def clean_temp_files(self, retention_hours=None):
        """æ¸…ç†ä¸´æ—¶æ–‡ä»¶"""
        retention_hours = retention_hours or self.cleanup_config['temp_file_retention_hours']
        cutoff_time = datetime.now() - timedelta(hours=retention_hours)

        logger.info(f"å¼€å§‹æ¸…ç† {retention_hours} å°æ—¶å‰çš„ä¸´æ—¶æ–‡ä»¶...")

        temp_patterns = [
            '**/*.tmp',
            '**/*.temp',
            '**/*~',
            '**/.DS_Store',
            '**/Thumbs.db'
        ]

        cleaned_count = 0
        for pattern in temp_patterns:
            for file_path in self.project_root.glob(pattern):
                try:
                    # è·³è¿‡é‡è¦ç›®å½•
                    if any(part.startswith('.') for part in file_path.parts):
                        continue

                    file_mtime = datetime.fromtimestamp(file_path.stat().st_mtime)

                    if file_mtime < cutoff_time:
                        file_path.unlink()
                        logger.info(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶: {file_path}")
                        cleaned_count += 1

                except Exception as e:
                    logger.warning(f"åˆ é™¤ä¸´æ—¶æ–‡ä»¶å¤±è´¥ {file_path}: {e}")

        logger.info(f"ä¸´æ—¶æ–‡ä»¶æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {cleaned_count} ä¸ªæ–‡ä»¶")
        return cleaned_count

    def clean_stock_cache(self):
        """æ¸…ç†è‚¡ç¥¨ç¼“å­˜æ–‡ä»¶"""
        logger.info("å¼€å§‹æ¸…ç†è‚¡ç¥¨ç¼“å­˜æ–‡ä»¶...")

        cache_files = [
            'smart_stock_universe.json',
            'stock_data_cache.json',
            'api_cache.json'
        ]

        cleaned_count = 0
        for cache_file in cache_files:
            cache_path = self.project_root / cache_file
            if cache_path.exists():
                try:
                    # æ£€æŸ¥ç¼“å­˜æ–‡ä»¶å¹´é¾„
                    file_age = time.time() - cache_path.stat().st_mtime
                    if file_age > 86400:  # 24å°æ—¶
                        cache_path.unlink()
                        logger.info(f"åˆ é™¤è¿‡æœŸç¼“å­˜: {cache_file}")
                        cleaned_count += 1
                    else:
                        logger.info(f"ç¼“å­˜æ–‡ä»¶ä»æ–°é²œï¼Œä¿ç•™: {cache_file}")
                except Exception as e:
                    logger.warning(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥ {cache_file}: {e}")

        logger.info(f"è‚¡ç¥¨ç¼“å­˜æ¸…ç†å®Œæˆï¼Œå…±åˆ é™¤ {cleaned_count} ä¸ªæ–‡ä»¶")
        return cleaned_count

    def get_disk_usage_stats(self):
        """è·å–ç£ç›˜ä½¿ç”¨ç»Ÿè®¡"""
        stats = {
            'total_size': 0,
            'directories': {}
        }

        important_dirs = ['src', 'core', 'scripts', 'logs', 'results', 'archive', 'venv']

        for dir_name in important_dirs:
            dir_path = self.project_root / dir_name
            if dir_path.exists():
                size = 0
                for file_path in dir_path.rglob('*'):
                    if file_path.is_file():
                        size += file_path.stat().st_size

                stats['directories'][dir_name] = {
                    'size': size,
                    'size_mb': round(size / (1024 * 1024), 2)
                }
                stats['total_size'] += size

        stats['total_size_mb'] = round(stats['total_size'] / (1024 * 1024), 2)
        return stats

    def generate_cleanup_report(self, cleanup_results):
        """ç”Ÿæˆæ¸…ç†æŠ¥å‘Š"""
        report = {
            'cleanup_time': datetime.now().isoformat(),
            'cleanup_results': cleanup_results,
            'disk_usage_before': cleanup_results.get('disk_usage_before', {}),
            'disk_usage_after': self.get_disk_usage_stats()
        }

        # è®¡ç®—èŠ‚çœçš„ç©ºé—´
        before_size = cleanup_results.get('disk_usage_before', {}).get('total_size', 0)
        after_size = report['disk_usage_after']['total_size']
        saved_space = before_size - after_size

        report['saved_space_mb'] = round(saved_space / (1024 * 1024), 2)

        # ä¿å­˜æŠ¥å‘Š
        report_path = self.logs_dir / f'cleanup_report_{datetime.now().strftime("%Y%m%d_%H%M%S")}.json'
        with open(report_path, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)

        logger.info(f"æ¸…ç†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_path}")
        return report

    def run_full_cleanup(self):
        """æ‰§è¡Œå®Œæ•´çš„ç³»ç»Ÿæ¸…ç†"""
        logger.info("å¼€å§‹æ‰§è¡Œå®Œæ•´ç³»ç»Ÿæ¸…ç†...")
        start_time = time.time()

        # è·å–æ¸…ç†å‰çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ
        disk_usage_before = self.get_disk_usage_stats()

        # æ‰§è¡Œå„é¡¹æ¸…ç†ä»»åŠ¡
        cleanup_results = {
            'disk_usage_before': disk_usage_before,
            'tasks': {}
        }

        try:
            # 1. æ¸…ç†Pythonç¼“å­˜
            cache_count = self.clean_python_cache()
            cleanup_results['tasks']['python_cache'] = {'cleaned_count': cache_count}

            # 2. å½’æ¡£æ—§ç»“æœæ–‡ä»¶
            archived_count = self.clean_old_results()
            cleanup_results['tasks']['old_results'] = {'archived_count': archived_count}

            # 3. å‹ç¼©æ—§æ—¥å¿—æ–‡ä»¶
            compressed_count = self.clean_old_logs()
            cleanup_results['tasks']['old_logs'] = {'compressed_count': compressed_count}

            # 4. æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            temp_count = self.clean_temp_files()
            cleanup_results['tasks']['temp_files'] = {'cleaned_count': temp_count}

            # 5. æ¸…ç†è‚¡ç¥¨ç¼“å­˜
            cache_count = self.clean_stock_cache()
            cleanup_results['tasks']['stock_cache'] = {'cleaned_count': cache_count}

        except Exception as e:
            logger.error(f"æ¸…ç†è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")

        # ç”Ÿæˆæ¸…ç†æŠ¥å‘Š
        cleanup_time = time.time() - start_time
        cleanup_results['total_time_seconds'] = round(cleanup_time, 2)

        report = self.generate_cleanup_report(cleanup_results)

        logger.info(f"ç³»ç»Ÿæ¸…ç†å®Œæˆï¼è€—æ—¶ {cleanup_time:.2f} ç§’")
        logger.info(f"èŠ‚çœç£ç›˜ç©ºé—´: {report['saved_space_mb']:.2f} MB")

        return report

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ§¹ è‚¡ç¥¨ç­›é€‰ç³»ç»Ÿæ¸…ç†å·¥å…·")
    print("=" * 50)

    cleaner = SystemCleanup()

    # æ˜¾ç¤ºæ¸…ç†å‰çš„ç£ç›˜ä½¿ç”¨æƒ…å†µ
    print("\nğŸ“Š æ¸…ç†å‰ç£ç›˜ä½¿ç”¨æƒ…å†µ:")
    disk_usage_before = cleaner.get_disk_usage_stats()
    for dir_name, info in disk_usage_before['directories'].items():
        print(f"  {dir_name}/: {info['size_mb']} MB")
    print(f"  æ€»è®¡: {disk_usage_before['total_size_mb']} MB")

    # æ‰§è¡Œæ¸…ç†
    print("\nğŸš€ å¼€å§‹æ¸…ç†...")
    report = cleaner.run_full_cleanup()

    # æ˜¾ç¤ºæ¸…ç†ç»“æœ
    print("\nğŸ“‹ æ¸…ç†ç»“æœ:")
    for task_name, result in report['cleanup_results']['tasks'].items():
        for key, value in result.items():
            print(f"  {task_name}: {value}")

    print(f"\nğŸ’¾ èŠ‚çœç£ç›˜ç©ºé—´: {report['saved_space_mb']:.2f} MB")
    print(f"â±ï¸  æ¸…ç†è€—æ—¶: {report['cleanup_results']['total_time_seconds']:.2f} ç§’")

    print("\nâœ… ç³»ç»Ÿæ¸…ç†å®Œæˆï¼")

if __name__ == "__main__":
    main()